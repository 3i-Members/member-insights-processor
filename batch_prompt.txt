# TASK — Parallelizing Member Insights Processor on GCP

We have a Python CLI tool called `member-insights-processor` that processes member/customer data from BigQuery, runs AI summarization (Claude, Gemini, OpenAI), and stores results in Supabase. It’s already Dockerized and takes CLI arguments like `--contact-id` and `--limit`.

Goal:
- Run multiple customers in parallel to reduce total processing time.
- Use Google Cloud Run Jobs as the execution environment (start here; Cloud Batch is a fallback).
- Each job should process one contact or a small batch of contacts.

Deliverables:
1. **Dockerfile Review**
   - Ensure the current Dockerfile is production-ready for Cloud Run Jobs (small image size, minimal layers).
   - Confirm environment variables from `.env` or GCP Secret Manager are loaded securely.

2. **Cloud Run Jobs Deployment**
   - Write a `gcloud run jobs create` deployment command (or Terraform/infra-as-code snippet) that:
     - Allocates configurable memory (default 8 GiB, up to 32 GiB).
     - Passes `--contact-id` to the CLI.
     - Uses the existing Docker image.

3. **Parallel Execution Script**
   - Provide a Bash or Python orchestration script that:
     - Reads a list of contact IDs (from file or BigQuery query output).
     - Executes N Cloud Run Jobs in parallel.
     - Waits for completion and logs results.

4. **Scaling Considerations**
   - Document how to adjust memory per job (`--memory` flag).
   - Note that memory is per-job and accumulates when running multiple in parallel.
   - Include guidance on when to switch to Cloud Batch if workload or runtime exceeds Cloud Run limits (1 hour execution time, 32 GiB RAM).

5. **Optional: Cloud Workflows**
   - Outline a Cloud Workflows YAML that triggers Cloud Run Jobs for each contact in parallel.

Constraints:
- No changes to core business logic.
- Keep solution lightweight — we don’t need a full orchestration framework at this stage.
- Assume the repo is already working locally with Docker.

Outcome:
We should be able to:
- Deploy the current processor to Cloud Run Jobs.
- Pass a single `--contact-id` per job.
- Trigger many jobs in parallel, each running independently with its own memory allocation.
- Reduce end-to-end runtime for processing large customer sets.
